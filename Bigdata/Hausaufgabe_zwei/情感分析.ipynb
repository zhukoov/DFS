{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout, Activation, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train_DataSet.csv')\n",
    "train_label = pd.read_csv('Train_DataSet_Label.csv')\n",
    "train = pd.merge(train_data, train_label, how='left', on='id')\n",
    "train = train[(train.label.notnull()) & (train.content.notnull())]\n",
    "test = pd.read_csv('Test_DataSet.csv')\n",
    "\n",
    "train['title'] = train['title'].fillna('')\n",
    "train['content'] = train['content'].fillna('')\n",
    "test['title'] = test['title'].fillna('')\n",
    "test['content'] = test['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter(text):\n",
    "    text = re.sub(\"[A-Za-z0-9\\!\\=\\？\\%\\[\\]\\,\\（\\）\\>\\<:&lt;\\/#\\. -----\\_]\", \"\", text)\n",
    "    text = text.replace('图片', '')\n",
    "    text = text.replace('\\xa0', '') # 删除nbsp\n",
    "    # new\n",
    "    r1 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, ' ', text)        #去除html标签\n",
    "    text = re.sub(r1,'',text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    data['title'] = data['title'].apply(lambda x: filter(x))\n",
    "    data['content'] = data['content'].apply(lambda x: filter(x))\n",
    "    return data\n",
    "train = clean_text(train)\n",
    "test = clean_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_table('stop.txt', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.setLogLevel(jieba.logging.INFO)\n",
    "import string\n",
    "table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "def cut_text(sentence):\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    # 去除停用词\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "#     # 去除英文标点\n",
    "#     tokens = [w.translate(table) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title = [cut_text(sent) for sent in train.title.values]\n",
    "train_content = [cut_text(sent) for sent in train.content.values]\n",
    "test_title = [cut_text(sent) for sent in test.title.values]\n",
    "test_content = [cut_text(sent) for sent in test.content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = train_title + train_content + test_title + test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "# model_word2vec = gensim.models.Word2Vec.load('final_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0min 6s\n"
     ]
    }
   ],
   "source": [
    "model_word2vec = gensim.models.Word2Vec(min_count=1, \n",
    "                                        window=5, \n",
    "                                        vector_size=256,\n",
    "                                        workers=4,\n",
    "                                        batch_words=1000)\n",
    "since = time.time()\n",
    "model_word2vec.build_vocab(all_doc, progress_per=2000)\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 4080627.25, time: 0min 9s\n",
      "Better model. Best loss: 4080627.25\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 2, loss: 2651312.75, time: 0min 9s\n",
      "Better model. Best loss: 2651312.75\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 3, loss: 2272688.00, time: 0min 9s\n",
      "Better model. Best loss: 2272688.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 4, loss: 1891435.00, time: 0min 9s\n",
      "Better model. Best loss: 1891435.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Epoch 5, loss: 1837068.00, time: 0min 9s\n",
      "Better model. Best loss: 1837068.00\n",
      "Model ./final_word2vec_model save done!\n",
      "Time to train: 0min 50s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "model_word2vec.train(all_doc, total_examples=model_word2vec.corpus_count, \n",
    "                        epochs=5, compute_loss=True, report_delay=60*10,\n",
    "                        callbacks=[EpochSaver('./final_word2vec_model')])\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to train: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_title + test_title)\n",
    "# tokenizer.fit_on_texts(train_content + test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32058/32058 [00:00<00:00, 306496.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 转化成词向量矩阵，利用新的word2vec模型\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "error_count=0\n",
    "embedding_matrix = np.zeros((vocab_size + 1, 256))\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    if model_word2vec.wv.__contains__(word):\n",
    "        embedding_matrix[i] = model_word2vec.wv[word]\n",
    "    else:\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(train_title)\n",
    "traintitle = pad_sequences(sequence, maxlen=30)\n",
    "sequence = tokenizer.texts_to_sequences(test_title)\n",
    "testtitle = pad_sequences(sequence, maxlen=30)\n",
    "# sequence = tokenizer.texts_to_sequences(train_content)\n",
    "# traincontent = pad_sequences(sequence, maxlen=512)\n",
    "# sequence = tokenizer.texts_to_sequences(test_content)\n",
    "# testcontent = pad_sequences(sequence, maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def metric_F1score(y_true,y_pred):    \n",
    "    TP=tf.reduce_sum(y_true*tf.round(y_pred))\n",
    "    TN=tf.reduce_sum((1-y_true)*(1-tf.round(y_pred)))\n",
    "    FP=tf.reduce_sum((1-y_true)*tf.round(y_pred))\n",
    "    FN=tf.reduce_sum(y_true*(1-tf.round(y_pred)))\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    F1score=2*precision*recall/(precision+recall)\n",
    "    return F1score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen,\n",
    "                              weights=[embedding_matrix])(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n",
    "    \n",
    "model = TextCNN(maxlen=30, max_features=len(tokenizer.word_index) + 1,\n",
    "                    embedding_dims=256, class_num=3, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy',metric_F1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 30, 256)      8207104     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 28, 128)      98432       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 27, 128)      131200      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 26, 128)      163968      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 128)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 128)         0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_5 (Global  (None, 128)         0           ['conv1d_5[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 384)          0           ['global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]', \n",
      "                                                                  'global_max_pooling1d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            1155        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,601,859\n",
      "Trainable params: 8,601,859\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['label'].astype(int)\n",
    "# labels = to_categorical(label) \n",
    "# train_X, val_X, train_Y, val_Y = train_test_split(traintitle, label, shuffle=True, test_size=0.2,\n",
    "#                                                     random_state=2019)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(traintitle, label, shuffle=True, test_size=0.2,\n",
    "                                                    random_state=2019)\n",
    "train_Y = to_categorical(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.8135 - accuracy: 0.6925 - metric_F1score: 0.6823\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.4078 - accuracy: 0.8417 - metric_F1score: 0.8386\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.2530 - accuracy: 0.9217 - metric_F1score: 0.9204\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.1637 - accuracy: 0.9609 - metric_F1score: 0.9625\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 8s 168ms/step - loss: 0.1017 - accuracy: 0.9823 - metric_F1score: 0.9824\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.0691 - accuracy: 0.9907 - metric_F1score: 0.9906\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.0493 - accuracy: 0.9928 - metric_F1score: 0.9929\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.0406 - accuracy: 0.9940 - metric_F1score: 0.9941\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.0381 - accuracy: 0.9943 - metric_F1score: 0.9945\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.0332 - accuracy: 0.9954 - metric_F1score: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a6808e788>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X,\n",
    "          train_Y,\n",
    "          batch_size=128,\n",
    "          epochs=10)\n",
    "# model.fit(traintitle,\n",
    "#           labels,\n",
    "#           batch_size=128,\n",
    "#           epochs=3,\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636989761999734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_val = model.predict(val_X)\n",
    "print(f1_score(val_Y, np.argmax(pred_val, axis=1), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(testtitle), axis=1)\n",
    "test['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4042\n",
       "2    2936\n",
       "0     378\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>016120c239f547ea8881ab632ddd03bb</td>\n",
       "      <td>沙湾职中代表队斩获全市中学生防震减灾知识竞赛高中组头筹</td>\n",
       "      <td>沙湾新闻网讯雷小军月日在市教育局和市防震减灾局共同主办的全市中学生防震减灾知识竞赛中沙湾职中...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>021b50be5c714739a4d5ac46567c03f2</td>\n",
       "      <td>兢兢业业的排头兵蒙牛集团一线员工风采录</td>\n",
       "      <td>随着消费升级节奏的加快消费者对乳品的需求也越来越来对于蒙牛牛奶的研发部门而言研究出更多高品质...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>026928bb2e5f4e1391fe7965ce949ebf</td>\n",
       "      <td>久治县组织干部观摩学习海南州贵德县民族团结进步创建工作先进经验</td>\n",
       "      <td>为进一步提升久治县创建民族团结进步先进县工作水平久治县委主动与海南州贵德县委共和县委联系对接...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>02af201cde4d4c62b7b28e3f54bb8d17</td>\n",
       "      <td>安康汉阴警方帮助农民工追回拖欠工资万元</td>\n",
       "      <td>下载次数下载附件保存到相册分钟前上传月日上午汉阴县公安局铁佛派出所民警在户籍大厅当场为十余名...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>02ee82f4e9bb49a0b87361dcff6fb1d0</td>\n",
       "      <td>九寨地震平安归来心有余悸为九寨祈福</td>\n",
       "      <td>晚上八点半左右我们一行四人达到了九寨沟沟口预订的酒店放下行李从酒店出来找到一家餐馆准备随便吃...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>fa1cb9f75bfa4bf38846e569d74711d0</td>\n",
       "      <td>中国大使与印尼穆斯林共同开斋并启动便民卫生项目</td>\n",
       "      <td>月日中国驻印尼大使肖千右来到位于雅加达南区的阿斯沙克法经学院与印尼最大穆斯林组织伊斯兰教士联...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7201</th>\n",
       "      <td>fa27b649587a498eb75780387b26b1ce</td>\n",
       "      <td>正能量交警推车助人群众纷纷点赞</td>\n",
       "      <td>今天一张女交警奋力推车的照片在微信朋友圈火热传播网友纷纷转发点赞并写下评论说推车的交警同志真...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>fb9abb737fc942ee8909a9268c3fb88e</td>\n",
       "      <td>定向挑战黎园奔跑材化学院社区文化节活动激情开赛</td>\n",
       "      <td>点击蓝字关注关注我们定向挑战黎园奔跑材化学院社区文化节活动激情开赛黎小材比赛即将开始我将全程...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>fe9bbce72c194aea90016898e7ef4c82</td>\n",
       "      <td>福建警方摧毁一个网络水军团伙炒作舆情余起</td>\n",
       "      <td>新华社福州月日电记者王成据福建省公安厅消息莆田警方侦破一起公安部挂牌督办的网络水军案件摧毁网...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>feffd6e29b9d4f4b85479e4d22feb606</td>\n",
       "      <td>防冻应急抢险物资准备就绪</td>\n",
       "      <td>常熟日报讯记者严婷市市政设施养护所全力开展防冻抗滑工作尽最大努力消除隐患确保在低温雨雪冰冻天...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id                            title  \\\n",
       "44    016120c239f547ea8881ab632ddd03bb      沙湾职中代表队斩获全市中学生防震减灾知识竞赛高中组头筹   \n",
       "69    021b50be5c714739a4d5ac46567c03f2              兢兢业业的排头兵蒙牛集团一线员工风采录   \n",
       "77    026928bb2e5f4e1391fe7965ce949ebf  久治县组织干部观摩学习海南州贵德县民族团结进步创建工作先进经验   \n",
       "88    02af201cde4d4c62b7b28e3f54bb8d17              安康汉阴警方帮助农民工追回拖欠工资万元   \n",
       "92    02ee82f4e9bb49a0b87361dcff6fb1d0                九寨地震平安归来心有余悸为九寨祈福   \n",
       "...                                ...                              ...   \n",
       "7199  fa1cb9f75bfa4bf38846e569d74711d0          中国大使与印尼穆斯林共同开斋并启动便民卫生项目   \n",
       "7201  fa27b649587a498eb75780387b26b1ce                  正能量交警推车助人群众纷纷点赞   \n",
       "7237  fb9abb737fc942ee8909a9268c3fb88e          定向挑战黎园奔跑材化学院社区文化节活动激情开赛   \n",
       "7316  fe9bbce72c194aea90016898e7ef4c82             福建警方摧毁一个网络水军团伙炒作舆情余起   \n",
       "7325  feffd6e29b9d4f4b85479e4d22feb606                     防冻应急抢险物资准备就绪   \n",
       "\n",
       "                                                content  label  \n",
       "44    沙湾新闻网讯雷小军月日在市教育局和市防震减灾局共同主办的全市中学生防震减灾知识竞赛中沙湾职中...      0  \n",
       "69    随着消费升级节奏的加快消费者对乳品的需求也越来越来对于蒙牛牛奶的研发部门而言研究出更多高品质...      0  \n",
       "77    为进一步提升久治县创建民族团结进步先进县工作水平久治县委主动与海南州贵德县委共和县委联系对接...      0  \n",
       "88    下载次数下载附件保存到相册分钟前上传月日上午汉阴县公安局铁佛派出所民警在户籍大厅当场为十余名...      0  \n",
       "92    晚上八点半左右我们一行四人达到了九寨沟沟口预订的酒店放下行李从酒店出来找到一家餐馆准备随便吃...      0  \n",
       "...                                                 ...    ...  \n",
       "7199  月日中国驻印尼大使肖千右来到位于雅加达南区的阿斯沙克法经学院与印尼最大穆斯林组织伊斯兰教士联...      0  \n",
       "7201  今天一张女交警奋力推车的照片在微信朋友圈火热传播网友纷纷转发点赞并写下评论说推车的交警同志真...      0  \n",
       "7237  点击蓝字关注关注我们定向挑战黎园奔跑材化学院社区文化节活动激情开赛黎小材比赛即将开始我将全程...      0  \n",
       "7316  新华社福州月日电记者王成据福建省公安厅消息莆田警方侦破一起公安部挂牌督办的网络水军案件摧毁网...      0  \n",
       "7325  常熟日报讯记者严婷市市政设施养护所全力开展防冻抗滑工作尽最大努力消除隐患确保在低温雨雪冰冻天...      0  \n",
       "\n",
       "[378 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'label']].to_csv('baseline4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
